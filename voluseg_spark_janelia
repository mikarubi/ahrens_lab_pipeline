#!/usr/bin/env python

# set up
import os
import sys
import time
import datetime
from subprocess import run, PIPE

if len(sys.argv) < 3:
    sys.exit('Usage: voluseg_spark_janelia n_nodes dir_output [n_hours] [n_tasks]')

n_workers = int(sys.argv[1])
dir_output = sys.argv[2]
if len(sys.argv) < 4:
    n_hours = 8
else:
    n_hours = int(sys.argv[3])
if len(sys.argv) < 5:
    n_tasks = 2
else:
    n_tasks = int(sys.argv[4])
print('Setting a runtime limit of %d hours.'%(n_hours))
print('Setting each executor to have %d tasks.'%(n_tasks))

#%%

# define spark commands
spark_path = os.path.join(os.getenv('SPARK_HOME'), 'bin')
spark_class = os.path.join(spark_path,'spark-class')
deploy_master = 'org.apache.spark.deploy.master.Master'
deploy_workers = 'org.apache.spark.deploy.worker.Worker'

# define local filenames
file_path = os.path.dirname(os.path.abspath(__file__))
spark_conf = os.path.join(file_path, 'spark_properties.conf')

# get job name
name_user = os.getenv('USER')
timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
name_job = 'voluseg_%s_%s'%(name_user, timestamp)

# define environment worker variables
os.environ['SPARK_WORKER_DIR'] = '/scratch/%s/%s'%(name_user,name_job)
os.environ['SPARK_LOCAL_DIRS'] = '/scratch/%s/%s'%(name_user,name_job)

name_master = 'master_%s'%(name_job)
name_workers = 'worker_%s[1-%d]'%(name_job,n_workers)

# launch master
launch_master = run([
    'bsub',
    '-n','1', # '-o','/dev/null',
    '-W','%d:00'%(n_hours),
    '-J',name_master,
    spark_class,deploy_master,
    '--properties-file',spark_conf])

# check master launched
if launch_master.returncode:
    sys.exit('Master was not launched.')

# get all spark job ids
while 1:
    time.sleep(5)
    bpeek = run(['bpeek','-J',name_master], stdout=PIPE)
    # if master launched
    if 'ALIVE' in bpeek.stdout.decode():
        url_master = 'spark://'+bpeek.stdout.decode().split('spark://')[1].split()[0]
        break
    else:
        print('Waiting for master.')

# launch workers
launch_workers = run([
    'bsub',
    '-n','32', # '-o','/dev/null',
    '-W','%d:00'%(n_hours),
    '-J',name_workers,
    spark_class,deploy_workers,
    '--properties-file',spark_conf,
    url_master])

#check workers launched
if launch_workers.returncode:
    run(['bkill','-J',name_master])
    sys.exit('Workers were not launched.')

while 1:
    time.sleep(5)
    worker_status = run(['bjobs','-J',name_workers], stdout=PIPE)
    # if any workers running
    if all([li.split()[2]=='RUN' for li in worker_status.stdout.decode().split('\n')[1:-1]]):
        break
    else:
        print('Waiting for workers.')

# run spark-submit with prepro_script
run([
     os.path.join(spark_path,'spark-submit'),
     '--master', url_master,
     '--properties-file', spark_conf,
     '--conf', 'spark.default.parallelism=%d'%(30 * n_workers * n_tasks),
     os.path.join(file_path, 'voluseg_submit.py'),
     dir_output, name_workers])

# terminate jobs
run(['bkill','-J',name_workers])
run(['bkill','-J',name_master])
