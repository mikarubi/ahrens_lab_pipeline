#!/usr/bin/env python

# set up
import os
import sys
import time
import datetime
from subprocess import run, PIPE

if len(sys.argv) < 3:
    sys.exit('Usage: voluseg_spark_janelia n_workers dir_output [n_hours]')

n_workers = sys.argv[1]
dir_output = sys.argv[2]
if len(sys.argv) < 4:
    print('Setting a default time-limit of 8 hours.')
    n_hours = '8'
else:
    n_hours = sys.argv[3]

#%%

# define spark commands
spark_path = os.path.join(os.getenv('SPARK_HOME'), 'bin')
spark_class = os.path.join(spark_path,'spark-class')
deploy_master = 'org.apache.spark.deploy.master.Master'
deploy_workers = 'org.apache.spark.deploy.worker.Worker'

# define local filenames
file_path = os.path.dirname(os.path.abspath(__file__))
spark_conf = os.path.join(file_path, 'spark_properties.conf')

# get job name
name_user = os.getenv('USER')
timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
name_job = 'voluseg_%s_%s'%(name_user, timestamp)

# define environment worker variables
os.environ['SPARK_WORKER_DIR'] = '/scratch/%s/%s'%(name_user,name_job)
os.environ['SPARK_LOCAL_DIRS'] = '/scratch/%s/%s'%(name_user,name_job)

name_master = 'master_%s'%(name_job)
name_workers = 'worker_%s[1-%s]'%(name_job,n_workers)

# launch master
launch_master = run([
    'bsub','-W%s:00'%(n_hours),
    '-J',name_master,'-n1',
    spark_class,deploy_master,
    '--properties-file',spark_conf])
# check master launched
if launch_master.returncode:
    sys.exit('Master was not launched.')

# get all spark job ids
while 1:
    time.sleep(5)
    bpeek = run(['bpeek','-J',name_master], stdout=PIPE)
    # if master launched
    if 'ALIVE' in bpeek.stdout.decode():
        url_master = 'spark://'+bpeek.stdout.decode().split('spark://')[1].split()[0]
        break
    else:
        print('Waiting for master.')

# launch workers
launch_workers = run([
    'bsub','-W%s:00'%(n_hours),
    '-J',name_workers,'-n32',
    spark_class,deploy_workers,
    '--properties-file',spark_conf,
    url_master])
#check workers launched
if launch_workers.returncode:
    run(['bkill','-J',name_master])
    sys.exit('Workers were not launched.')

while 1:
    time.sleep(5)
    worker_status = run(['bjobs','-J',name_workers], stdout=PIPE)
    # if any workers running
    if all([li.split()[2]=='RUN' for li in worker_status.stdout.decode().split('\n')[1:-1]]):
        break
    else:
        print('Waiting for workers.')

# run spark-submit with prepro_script
run([
     os.path.join(spark_path,'spark-submit'),
     '--master', url_master,
     '--properties-file', spark_conf,
     '--conf', 'spark.default.parallelism=%d'%(30 * int(n_workers)),
     os.path.join(file_path, 'voluseg_submit.py'),
     dir_output])
# NB: 60 = 2 * (spark.executor.cores).
# spark.executor.cores=30 is defined in spark_properties.conf
# the above line implies that every worker node will run 30 cores.
# therefore, (60 * int(n_workers)) implies that each core has two tasks


# terminate jobs
run(['bkill','-J',name_workers])
run(['bkill','-J',name_master])
